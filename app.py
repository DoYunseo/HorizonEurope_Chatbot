from langchain.prompts import PromptTemplate
import streamlit as st
from langchain.prompts import MessagesPlaceholder
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
import time
import dotenv
from googlesearch import search

dotenv.load_dotenv()

# ì›¹ì‚¬ì´íŠ¸ ë©”ì¸ í—¤ë”
st.title("Horizon Europe Chatbot ğŸ‡ªğŸ‡º")
st.write("ì´ ì±—ë´‡ì€ Horizon Europe í™ˆí˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
st.write("ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´, ì˜ˆ: '0000 í”„ë¡œì íŠ¸ ì›¹ì‚¬ì´íŠ¸ ì•Œë ¤ì¤˜'ì™€ ê°™ì´ ì§ˆë¬¸í•˜ì„¸ìš”.")


# ì„¸ì…˜ ì´ˆê¸°í™”: ëª¨ë¸ê³¼ ë©”ì‹œì§€ ì €ì¥
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-4"

if "messages" not in st.session_state:
    st.session_state["messages"] = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥

# ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
vector_store_path = "new_vector_store"
embedding = OpenAIEmbeddings(model="text-embedding-ada-002")
vector_store = FAISS.load_local(vector_store_path, embedding, allow_dangerous_deserialization=True)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ë‹¹ì‹ ì€ ë‹¤êµ­ì–´ë¥¼ ì§€ì›í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ì§ˆë¬¸ì˜ ì–¸ì–´ê°€ í•œêµ­ì–´ì´ë©´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ì˜ì–´ì´ë©´ ì˜ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
        "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹µë³€ì€ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. "
        "ë¬¸ë§¥:\n{context}\n\n"
        "ì§ˆë¬¸: {question}\n\n"
        "ë‹µë³€:"
    )
)

# êµ¬ê¸€ ê²€ìƒ‰ì„ í†µí•´ í”„ë¡œì íŠ¸ ì›¹ì‚¬ì´íŠ¸ ë§í¬ ê°€ì ¸ì˜¤ê¸°
def get_project_website(project_name):
    query = f"{project_name} project website"
    search_results = list(search(query, num_results=5))  # ì—¬ëŸ¬ ê°œì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìœ íš¨í•œ ì›¹ì‚¬ì´íŠ¸ ë§í¬ë§Œ í•„í„°ë§ (ë„ë©”ì¸ ì˜ˆì‹œ: .eu, .com, .org, .edu ë“±)
    valid_domains = [ ".eu", ".com", ".org", ".edu"]
    for result in search_results:
        if any(domain in result for domain in valid_domains):
            return result  # ìœ íš¨í•œ ë„ë©”ì¸ì´ ë°œê²¬ë˜ë©´ ë°˜í™˜
    return "ìœ íš¨í•œ ì›¹ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(f"{message['content']}")

# ìœ ì € ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):

    # ìœ ì € ì…ë ¥ì„ í™”ë©´ì— í‘œì‹œí•˜ê³  ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(f"{prompt}")

    # ì‚¬ìš©ìê°€ "ì›¹ì‚¬ì´íŠ¸" ê´€ë ¨ ì§ˆë¬¸ì„ í–ˆì„ ë•Œ
    if "ì›¹ì‚¬ì´íŠ¸" in prompt:
        # í”„ë¡œì íŠ¸ ì´ë¦„ ì¶”ì¶œ: "ì›¹ì‚¬ì´íŠ¸" ì•ì˜ ë‹¨ì–´ë¥¼ í”„ë¡œì íŠ¸ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        project_name = prompt.split("ì›¹ì‚¬ì´íŠ¸")[0].strip()
        website = get_project_website(project_name)
        
        st.session_state["messages"].append({"role": "assistant", "content": f"í”„ë¡œì íŠ¸ ì›¹ì‚¬ì´íŠ¸ ë§í¬: {website}"})
        with st.chat_message("assistant"):
            st.markdown(f"ì›¹ì‚¬ì´íŠ¸ ë§í¬: {website}")
    else:
        # AI ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # íˆìŠ¤í† ë¦¬ë¥¼ ChatOpenAIì— ì „ë‹¬
            llm = ChatOpenAI(model=st.session_state["openai_model"])
            retriever = vector_store.as_retriever(search_kwargs={"k": 5})
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": prompt_template}
            )

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ëª¨ë“  ë©”ì‹œì§€ë¥¼ ìœ ì§€í•˜ê³ , ìµœê·¼ ë©”ì‹œì§€ë¥¼ í¬í•¨í•œ ì§ˆì˜ ìƒì„±
            query_with_history = (
                "ëŒ€í™” ë‚´ìš©:\n"
                + "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state["messages"]])
                + f"\n\nì§ˆë¬¸: {prompt}"
            )
            
            result = qa_chain({"query": query_with_history})
            full_response = result["result"]

            # í•œ ê¸€ìì”© ì¶œë ¥í•˜ì—¬ ì‹¤ì‹œê°„ íƒ€ì´í•‘ íš¨ê³¼ êµ¬í˜„
            displayed_text = ""
            for char in full_response:
                displayed_text += char
                message_placeholder.markdown(displayed_text + "â–Œ")  # ì»¤ì„œ íš¨ê³¼ ì¶”ê°€
                time.sleep(0.02)  # íƒ€ì´í•‘ ì†ë„ ì¡°ì ˆ (0.02ì´ˆ ê°„ê²©)

            message_placeholder.markdown(full_response)  # ìµœì¢… ê²°ê³¼ í‘œì‹œ

        # AI ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state["messages"].append({"role": "assistant", "content": full_response})
